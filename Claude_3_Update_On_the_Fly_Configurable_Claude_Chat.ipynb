{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs4PvT4pWXZTabdcKINgxJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiyoungkim/prompt_engineering_projects/blob/main/Claude_3_Update_On_the_Fly_Configurable_Claude_Chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic\n",
        "!pip install colorama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFmUdDGB_0SJ",
        "outputId": "37561f79-56c0-4360-9fe9-b242678fd347"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.10/dist-packages (0.18.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (2.6.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.16.3)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.0.7)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (0.4.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YLxBMnsh_mtD"
      },
      "outputs": [],
      "source": [
        "intent_detector_prompt = '''\n",
        "Your role is to detect the intent of the incoming message I will share with you. The intents will be categorized among the following categories:\n",
        "1 Settings Configuration\n",
        "2 Prompt Message\n",
        "3 Reset History\n",
        "4 End Conversation\n",
        "\n",
        "Only respond with the number corresponding to the intent.\n",
        "\n",
        "Categorize this message:\n",
        "{{MESSAGE}}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "configurator_prompt = '''\n",
        "Your role is to parse the given input to output a json object with only the following fields:\n",
        "- max_tokens\n",
        "- temperature\n",
        "- system_prompt\n",
        "\n",
        "The current values of these properties are:\n",
        "max_tokens = {{MAX_TOKENS}}\n",
        "temperature = {{TEMPERATURE}}\n",
        "system_prompt = {{SYSTEM_PROMPT}}\n",
        "\n",
        "Ignore any information not relevant to the properties.\n",
        "Unless you are outputting \"Done\", you can only output a json.\n",
        "When the user says they are finished configurating, your output to this question should be only the word \"Done\".\n",
        "\n",
        "Parse the following:\n",
        "{{INPUT}}\n",
        "'''"
      ],
      "metadata": {
        "id": "pg5vQQd9_wOo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import anthropic\n",
        "import time\n",
        "import json\n",
        "from google.colab import userdata\n",
        "from colorama import Fore, Back, Style\n",
        "\n",
        "def configurable_claude():\n",
        "\n",
        "  client = anthropic.Anthropic(\n",
        "      api_key = userdata.get(\"CLAUDE_API_KEY\")\n",
        "  )\n",
        "\n",
        "  # Conversation History\n",
        "  conversation_history = []\n",
        "\n",
        "  # Diagnostic Conversation History (For final output, just to have full history)\n",
        "  diagnostic_conversation_history = []\n",
        "\n",
        "  # Configuration Variables (Note this should really only impact the conversation)\n",
        "  MAX_TOKENS = 1024\n",
        "  TEMPERATURE = 1\n",
        "  SYSTEM_PROMPT = \"\"\n",
        "\n",
        "  # May be configured later? But thought I'd put it here\n",
        "  MODEL = \"claude-3-opus-20240229\"\n",
        "\n",
        "  def intent_detect(message):\n",
        "    prompt = intent_detector_prompt.replace(\"{{MESSAGE}}\", message)\n",
        "    return client.messages.create(model=MODEL,\n",
        "                                max_tokens=1024,\n",
        "                                messages=[{\"role\": \"user\", \"content\": prompt},])\n",
        "\n",
        "  def conversation(history):\n",
        "    return client.messages.create(model=MODEL,\n",
        "                                system=SYSTEM_PROMPT,\n",
        "                                max_tokens=MAX_TOKENS,\n",
        "                                temperature=TEMPERATURE,\n",
        "                                messages=history)\n",
        "\n",
        "  def configurator(prompt, message):\n",
        "    input_prompt = prompt.replace(\"{{INPUT}}\", message)\n",
        "    return client.messages.create(model=MODEL,\n",
        "                                max_tokens=1024,\n",
        "                                temperature=TEMPERATURE,\n",
        "                                messages=[{\"role\":\"user\", \"content\": input_prompt},])\n",
        "\n",
        "  while True:\n",
        "    # USER INPUT: CONVERSATION\n",
        "    print(Fore.GREEN)\n",
        "    message = input(\"User: \")\n",
        "    print(Style.RESET_ALL)\n",
        "\n",
        "    # Intent detection\n",
        "    intent_response = intent_detect(message)\n",
        "    intent = intent_response.content[0].text # Extract text\n",
        "\n",
        "    # Match intent to action\n",
        "    match intent:\n",
        "\n",
        "      # Configurator (May need to revisit and consider adding a history)\n",
        "      case \"1\":\n",
        "        # Record the configuration message\n",
        "        diagnostic_conversation_history.append({\"role\":\"user\",\"content\": message,\"max_tokens\": MAX_TOKENS, \"temperature\": TEMPERATURE, \"system_prompt\": SYSTEM_PROMPT, \"context\": \"configuration\"})\n",
        "\n",
        "        # Little message because otherwise it's confusing\n",
        "        print(Fore.BLACK + Back.WHITE + \"System Tooltip:\" + Style.RESET_ALL + \" To change a property, please say the property and the value you want to change it to. You can also ask for the current value of the property.\")\n",
        "        print(Style.BRIGHT + \"When you want to go back to the conversation, please say so, such as by saying \\\"Done\\\".\" + Style.RESET_ALL)\n",
        "\n",
        "        # Core configurator loop\n",
        "        while True:\n",
        "          # Update input prompt\n",
        "          config_prompt = configurator_prompt.replace(\"{{MAX_TOKENS}}\", str(MAX_TOKENS)).replace(\"{{TEMPERATURE}}\", str(TEMPERATURE)).replace(\"{{SYSTEM_PROMPT}}\", str(SYSTEM_PROMPT))\n",
        "\n",
        "          # USER INPUT: CONFIGURATION\n",
        "          print(Fore.GREEN)\n",
        "          config_input = input(\"User Configuration Request: \")\n",
        "          print(Style.RESET_ALL)\n",
        "\n",
        "          # AI Response\n",
        "          config_response = configurator(config_prompt, config_input)\n",
        "          config_output = config_response.content[0].text # Extract text\n",
        "\n",
        "          # Record User input and AI Response\n",
        "          diagnostic_conversation_history.append({\"role\":\"user\",\"content\": config_input,\"max_tokens\": MAX_TOKENS, \"temperature\": TEMPERATURE, \"system_prompt\": SYSTEM_PROMPT, \"context\": \"configuration\"})\n",
        "          diagnostic_conversation_history.append({\"role\":\"assistant\",\"content\": config_output,\"max_tokens\": MAX_TOKENS, \"temperature\": TEMPERATURE, \"system_prompt\": SYSTEM_PROMPT, \"context\": \"configuration\"})\n",
        "          # Escape configurator back to conversation\n",
        "          if config_output == \"Done\":\n",
        "            print(\"System: Thanks! Taking you back to the conversation now.\")\n",
        "            break\n",
        "\n",
        "          # CLAUDE OUTPUT: JSON output + update\n",
        "          else:\n",
        "            # Sometimes there's a random JSON output error, so I wanted to just have a catch-all and notification\n",
        "            try:\n",
        "                config_json = json.loads(config_output)\n",
        "                print(Fore.BLACK + Back.WHITE + \"JSON Output:\\n\" + Style.RESET_ALL + config_output)\n",
        "                # Update properties\n",
        "                if config_json[\"max_tokens\"] != MAX_TOKENS:\n",
        "                  MAX_TOKENS = config_json[\"max_tokens\"]\n",
        "                  print(\"System: Updated MAX_TOKENS!\")\n",
        "                if config_json[\"temperature\"] != TEMPERATURE:\n",
        "                  TEMPERATURE = config_json[\"temperature\"]\n",
        "                  print(\"System: Updated TEMPERATURE!\")\n",
        "                if config_json[\"system_prompt\"] != SYSTEM_PROMPT:\n",
        "                  SYSTEM_PROMPT = config_json[\"system_prompt\"]\n",
        "                  print(\"System: Updated SYSTEM_PROMPT!\")\n",
        "            except:\n",
        "              error_string = \"JSON Output Error. See context above.\"\n",
        "              diagnostic_conversation_history.append({\"role\":\"assistant\",\"content\": error_string,\"max_tokens\": MAX_TOKENS, \"temperature\": TEMPERATURE, \"system_prompt\": SYSTEM_PROMPT, \"context\": \"error\"})\n",
        "\n",
        "      # Conversation\n",
        "      case \"2\":\n",
        "        # Feed conversation history with User input into AI\n",
        "        conversation_history.append({\"role\":\"user\",\"content\": message})\n",
        "        conversation_response = conversation(conversation_history)\n",
        "        conversation_response_text = conversation_response.content[0].text # Extract text\n",
        "\n",
        "        # Update conversation history with AI response\n",
        "        conversation_history.append({\"role\":\"assistant\",\"content\": conversation_response_text})\n",
        "\n",
        "        # Update diagnostics\n",
        "        diagnostic_conversation_history.append({\"role\":\"user\",\"content\": message,\"max_tokens\": MAX_TOKENS, \"temperature\": TEMPERATURE, \"system_prompt\": SYSTEM_PROMPT, \"context\": \"conversation\"})\n",
        "        diagnostic_conversation_history.append({\"role\":\"assistant\",\"content\": conversation_response_text,\"max_tokens\": MAX_TOKENS, \"temperature\": TEMPERATURE, \"system_prompt\": SYSTEM_PROMPT, \"context\": \"conversation\"})\n",
        "\n",
        "        # CLAUDE OUTPUT: CONVERSATION\n",
        "        print(\"Claude: \" + conversation_response_text)\n",
        "\n",
        "      # Reset Conversation\n",
        "      case \"3\":\n",
        "        conversation_history = []\n",
        "\n",
        "        # But hold onto this for diagnostics\n",
        "        diagnostic_conversation_history.append({\"role\":\"user\",\"content\": message,\"max_tokens\": MAX_TOKENS, \"temperature\": TEMPERATURE, \"system_prompt\": SYSTEM_PROMPT, \"context\": \"reset\"})\n",
        "\n",
        "        # Let the user know\n",
        "        print(\"System: Conversation has been reset!\")\n",
        "\n",
        "      # End Conversation\n",
        "      case \"4\":\n",
        "\n",
        "        print(\"Thank you! I will output the diagnostics for this conversation now after 3 seconds.\")\n",
        "        time.sleep(0.5)\n",
        "\n",
        "        for i in range(3,0,-1):\n",
        "          print(str(i) + \"...\")\n",
        "          time.sleep(1)\n",
        "\n",
        "        # For fun, hold this last message for diagnostics\n",
        "        diagnostic_conversation_history.append({\"role\":\"user\",\"content\": message,\"max_tokens\": MAX_TOKENS, \"temperature\": TEMPERATURE, \"system_prompt\": SYSTEM_PROMPT, \"context\": \"end\"})\n",
        "\n",
        "        # Output the conversation history\n",
        "        print(Fore.BLACK + Back.WHITE + \"\\n\\n===========Full Diagnostic Conversation History===========\")\n",
        "        print(Style.RESET_ALL)\n",
        "\n",
        "        for line in diagnostic_conversation_history:\n",
        "\n",
        "          # Output formatting\n",
        "          output_prefix = \"\"\n",
        "          if line[\"role\"] == \"user\":\n",
        "            output_prefix = Fore.GREEN + \"\\n\\nUser: \"\n",
        "          elif line[\"role\"] == \"assistant\":\n",
        "            output_prefix = \"\\n\\nClaude: \"\n",
        "          print(output_prefix + line[\"content\"] + Style.RESET_ALL)\n",
        "\n",
        "          # Fun formatting\n",
        "          message_format = \"\"\n",
        "          match line[\"context\"]:\n",
        "            case \"configuration\":\n",
        "              message_format = Fore.YELLOW\n",
        "            case \"conversation\":\n",
        "              message_format = Fore.BLUE\n",
        "            case \"reset\":\n",
        "              message_format = Fore.RED\n",
        "            case \"end\":\n",
        "              message_format = Fore.MAGENTA\n",
        "            case \"error\":\n",
        "              message_format = Fore.RED + Style.BRIGHT\n",
        "\n",
        "          # Configuration and context for each message\n",
        "          print(message_format + \"max_tokens: \" + str(line[\"max_tokens\"]) + \", temperature: \" + str(line[\"temperature\"]) + \", system prompt: \" + str(line[\"system_prompt\"]) + \", context: \" + str(line[\"context\"]) + \"\\n\\n\" + Style.RESET_ALL)\n",
        "\n",
        "        # Output current configuration\n",
        "        print(\"Current Max Tokens: \",MAX_TOKENS)\n",
        "        print(\"Current Temperature: \",TEMPERATURE)\n",
        "        print(\"Current System Prompt: \",SYSTEM_PROMPT)\n",
        "\n",
        "        return conversation_history, diagnostic_conversation_history\n",
        "\n",
        "      case _:\n",
        "        # Just in case\n",
        "\n",
        "        # But hold onto this for diagnostics and error checking\n",
        "        diagnostic_conversation_history.append({\"role\":\"user\",\"content\": message,\"max_tokens\": MAX_TOKENS, \"temperature\": TEMPERATURE, \"system_prompt\": SYSTEM_PROMPT, \"context\": \"error\"})\n",
        "        diagnostic_conversation_history.append({\"role\":\"assistant\",\"content\": intent,\"max_tokens\": MAX_TOKENS, \"temperature\": TEMPERATURE, \"system_prompt\": SYSTEM_PROMPT, \"context\": \"error\"})\n",
        "\n",
        "        # Print it, but we should be able to go back up top\n",
        "        print(Fore.RED + \"Unexpected intent detected! Intent Detection response was:\\n\" + Style.RESET_ALL + intent)\n",
        "\n",
        "  # Just because\n",
        "  return -1"
      ],
      "metadata": {
        "id": "dSw5yCsZ_xVU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convo_history, diagnostic_convo_history = configurable_claude()\n",
        "print(convo_history)\n",
        "print(diagnostic_convo_history)"
      ],
      "metadata": {
        "id": "gcpo5dzDAG-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hdWVp23lqu_U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}